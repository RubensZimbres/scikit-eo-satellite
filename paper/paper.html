<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-05-19">

<title>scikit-eo: A Python package for Remote Sensing Data Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="paper_files/libs/clipboard/clipboard.min.js"></script>
<script src="paper_files/libs/quarto-html/quarto.js"></script>
<script src="paper_files/libs/quarto-html/popper.min.js"></script>
<script src="paper_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="paper_files/libs/quarto-html/anchor.min.js"></script>
<link href="paper_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="paper_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="paper_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="paper_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="paper_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">scikit-eo: A Python package for Remote Sensing Data Analysis</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    Yonatan Tarazona <a href="https://orcid.org/0000-0002-5208-1004" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            1
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    Jakub Nowosad <a href="https://orcid.org/0000-0002-1057-3721" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            2
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 19, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<!-- #TODO -->
<ol type="1">
<li>Fix the data paths and make a folder to refer a sample data for testing and review.</li>
<li>Finish the writing - Add examples related to data fusion and RS Trends.</li>
<li>Link Documentation</li>
</ol>
<!-- #region -->
<section id="summary-propose" class="level1">
<h1>Summary &amp; Propose</h1>
<p>Remote sensing and, more importantly, the growing accessibility of remotely sensed data, has fundamentally transformed our capacity to comprehend, classify, and monitor the connected environmental conditions of our planet. This enhanced understanding has enables researchers to address and tackle a broader range of challenges effectively, by using novel tools and data at multiple scales that help to shed new light into the underlying factors that contribute to both local and global issues.</p>
<p>As more accessible the data is, the bigger the need is for open-source tools to read, process and execute analysis that contribute to underpin patterns, changes and trends that are critical for environmental studies. Applications that integrate spatial-temporal data are currently used for several purposes, such as monitoring and assessment land cover changes <span class="citation" data-cites="Liping2018">(<a href="#ref-Liping2018" role="doc-biblioref">Liping, Yujun, and Saeed 2018</a>)</span>, deforestation <span class="citation" data-cites="Blanc2016">(<a href="#ref-Blanc2016" role="doc-biblioref">Blanc, Gond, and Minh 2016</a>)</span>, impact on urbanization level <span class="citation" data-cites="Trinder2020">(<a href="#ref-Trinder2020" role="doc-biblioref">Trinder and Liu 2020</a>)</span>, climate change impacts <span class="citation" data-cites="Yang2013">(<a href="#ref-Yang2013" role="doc-biblioref">Yang et al. 2013</a>)</span>, water quality <span class="citation" data-cites="Wang2019">(<a href="#ref-Wang2019" role="doc-biblioref">Wang and Yang 2019</a>)</span>, air pollution <span class="citation" data-cites="Seham2022">(<a href="#ref-Seham2022" role="doc-biblioref">Al-Alola et al. 2022</a>)</span>, biodiversity conservation <span class="citation" data-cites="Jeannine2022">(<a href="#ref-Jeannine2022" role="doc-biblioref">Cavender-Bares et al. 2022</a>)</span>, and natural disaster management <span class="citation" data-cites="Kucharczyk2021">(<a href="#ref-Kucharczyk2021" role="doc-biblioref">Kucharczyk and Hugenholtz 2021</a>)</span>, to name a few. By collecting data over large periods of time researchers can identify or measure the impact of human activities on ecosystems and support the data-driven decision-making process for more sustainable resource management <span class="citation" data-cites="Jensen2000">(<a href="#ref-Jensen2000" role="doc-biblioref">Jensen 2000</a>)</span>.</p>
<p>But more availability of data, does not necessary more easy ways to process the data. Analysts spend an important among of time time finding the correct libraries that allow them to read and process the data. Tools for mapping land degradation through linear trend of vegetation indices <span class="citation" data-cites="TARAZONA2018367">(<a href="#ref-TARAZONA2018367" role="doc-biblioref">Tarazona, Mantas, and Pereira 2018</a>)</span>, <span class="citation" data-cites="Tarazona2022">(<a href="#ref-Tarazona2022" role="doc-biblioref">Tarazona 2022</a>)</span>, data fusion process for optical and radar images to quickly classify the vegetation cover, and the integration of machine learning techniques are still separate in multiple libraries, along with diffuse documentation that limits the analysis where the main concerns of the causes or patterns of the environmental conditions should be the main focus.</p>
<p>Therefore, <strong>scikit-eo</strong> is a Python package that provides the necessary tools for remote sensing analysis. It is a centralized, scalable and open source toolkit, developed to fill the gaps in remotely sensed data processing tools. This toolkit can be use in multiple venues from a lecturer room as a toolkit for introduction of programming skills using python and remote sensing for environmental studies, or can be use as part of any Python setting in a research project. The majority of the tools including in <strong>scikit-eo</strong> are derived from scientific publications, while others are valuable algorithms that streamline data processing into just a few lines of code. By integrating this set of diverse tools, users can focus their time and energy on analyzing the results of their data, rather than getting bogged down in the intricacies lines of code. By centralizing, integrating use cases and example of data, <strong>scikit-eo</strong> serves as a way to allow researchers to optimize their resources and dedicate their focus to the meaningful interpretation of their findings with greater efficiency, rather to stress out with coding task.</p>
</section>
<section id="highlights" class="level1">
<h1>Highlights</h1>
<p><strong>scikit-eo</strong> is an open-source toolkit built entirely in Python that provides a variety of remote sensing tools, from basic and exploratory functions to more advanced methods to classify, calibrate, or fusion satellite imagery. Depending on users’ needs <strong>scikit-eo</strong> can provide the basic but essential land cover characterization mapping including the confusion matrix and the required metrics. User’s accuracy, producer’s accuracy, omissions and commissions are required metrics that users can get in a form of a pandas <code>DataFrame</code> object. Using <strong>scikit-eo</strong> users can produce a predicted classes map, i.e., a land cover map which represents the output of the classification algorithm or the output of the segmentation algorithm. These two outcomes must be generated along with the uncertainties with a confidence interval (<span class="math inline">\(95\)</span>% or <span class="math inline">\(90\)</span>%). All required metrics can be obtained with <strong>scikit-eo</strong>. There are other useful tools for remote sensing analysis that can be found in this package, for more information about the full list of the supported function as well how to install and execute the package within a python setting visit <a href="https://ytarazona.github.io/scikit-eo/">scikit-eo website</a></p>
</section>
<section id="audience" class="level1">
<h1>Audience</h1>
<p><strong>scikit-eo</strong> is a versatile Python package designed to cover to a wide range of users, including students, professionals on remote sensing, researchers on environmental analysis, and organizations looking for satellite image processing and analysis. Its comprehensive features make it well-suited for various applications, such as university teaching, lectures that include technical and practical sessions, and cutting-edge research using the most recent machine learning and deep learning techniques applied in the field of remote sensing. Whether the users is a student seeking to get insights from a satellite image analysis or a experienced researcher looking for advanced tools, <strong>scikit-eo</strong> offers a valuable resource to support the most valuable methods for environmental studies.</p>
<section id="scikit-eo-as-research-tool" class="level3">
<h3 class="anchored" data-anchor-id="scikit-eo-as-research-tool"><strong>scikit-eo</strong> as Research tool:</h3>
<p>advance techniques for research proposes.</p>
</section>
<section id="scikit-eo-in-the-lecture-room" class="level3">
<h3 class="anchored" data-anchor-id="scikit-eo-in-the-lecture-room"><strong>scikit-eo</strong> in the lecture room:</h3>
<p>Simple methods for intro and RS practice, as well programming skills.</p>
</section>
<section id="scikit-eo-as-open-source-tool" class="level3">
<h3 class="anchored" data-anchor-id="scikit-eo-as-open-source-tool"><strong>scikit-eo</strong> as open source tool:</h3>
<p>Scalable, open and well documented tool.</p>
</section>
</section>
<section id="functionalities" class="level1">
<h1>Functionalities</h1>
<section id="main-tools" class="level2">
<h2 class="anchored" data-anchor-id="main-tools">Main tools</h2>
<p><strong>Scikit-eo</strong> comes with several algorithms to process satellite images in order to study different environmental issues. Atmospheric correction, machine learning and deep learning, estimating area and uncertainty, linear trend, combining optical and radar images, among others, are some main functions listed below:</p>
<table class="table">
<caption>Main tools available for <strong>scikit-eo</strong> package. </caption>
<colgroup>
<col style="width: 20%">
<col style="width: 79%">
</colgroup>
<thead>
<tr class="header">
<th>Name of functions</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong><code>mla</code></strong></td>
<td>Supervised Classification in Remote Sensing</td>
</tr>
<tr class="even">
<td><strong><code>calmla</code></strong></td>
<td>Calibrating Supervised Classification in Remote Sensing</td>
</tr>
<tr class="odd">
<td><strong><code>confintervalML</code></strong></td>
<td>Information of Confusion Matrix by proportions of area, overall accuracy, user’s accuracy with confidence interval and estimated area with confidence interval as well.</td>
</tr>
<tr class="even">
<td><strong><code>deepLearning</code></strong></td>
<td>Deep Learning algorithms</td>
</tr>
<tr class="odd">
<td><strong><code>atmosCorr</code></strong></td>
<td>Radiometric and Atmospheric Correction</td>
</tr>
<tr class="even">
<td><strong><code>rkmeans</code></strong></td>
<td>K-means classification</td>
</tr>
<tr class="odd">
<td><strong><code>calkmeans</code></strong></td>
<td>This function allows to calibrate the kmeans algorithm. It is possible to obtain the best k value and the best embedded algorithm in kmeans.</td>
</tr>
<tr class="even">
<td><strong><code>pca</code></strong></td>
<td>Principal Components Analysis</td>
</tr>
<tr class="odd">
<td><strong><code>linearTrend</code></strong></td>
<td>Linear trend is useful for mapping forest degradation or land degradation</td>
</tr>
<tr class="even">
<td><strong><code>fusionrs</code></strong></td>
<td>This algorithm allows to fusion images coming from different spectral sensors (e.g., optical-optical, optical and SAR or SAR-SAR). Among many of the qualities of this function, it is possible to obtain the contribution (%) of each variable in the fused image</td>
</tr>
<tr class="odd">
<td><strong><code>sma</code></strong></td>
<td>Spectral Mixture Analysis - Classification sup-pixel</td>
</tr>
<tr class="even">
<td><strong><code>tassCap</code></strong></td>
<td>The Tasseled-Cap Transformation</td>
</tr>
</tbody>
</table>
<p>Of course, there are others functions will be found in the package. <!-- #endregion --></p>
</section>
<section id="brief-examples" class="level2">
<h2 class="anchored" data-anchor-id="brief-examples">Brief examples</h2>
<section id="example-01" class="level3">
<h3 class="anchored" data-anchor-id="example-01">Example 01</h3>
<p>An example of ML will be obtain. For this, Landsat-8 OLI (Operational Land Imager) will be used to classify using the Random Forest (RF) classifier. The datasets to be used in these examples can be downloaded <a href="https://drive.google.com/drive/folders/193RhNpACu9THcOZu8OzMh-btnFCOgHrU?usp=sharing">here</a>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 01. Libraries to used in these examples</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rasterio</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dbfread <span class="im">import</span> DBF</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scikeo.mla <span class="im">import</span> MLA</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scikeo.fusionrs <span class="im">import</span> fusionrs</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scikeo.calmla <span class="im">import</span> calmla</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 02. Image and endmembers</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>path_raster <span class="op">=</span> <span class="st">"\data\LC08_232066_20190727_SR.tif"</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> rasterio.<span class="bu">open</span>(path_raster)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>path_endm <span class="op">=</span> <span class="st">"\data\endmembers\endmembers.dbf"</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>endm <span class="op">=</span> DBF(path_endm)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 03. Instance of mla()</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>inst <span class="op">=</span> MLA(image <span class="op">=</span> img, endmembers <span class="op">=</span> endm)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 04. Applying RF with 70% of data to train</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>rf_class <span class="op">=</span> inst.RF(training_split <span class="op">=</span> <span class="fl">0.7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Classification results:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="scikit_eo_00.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption class="figure-caption">Original image and Image classified in the left and right panel respectively. </figcaption><p></p>
</figure>
</div>
</section>
<section id="example-02" class="level3">
<h3 class="anchored" data-anchor-id="example-02">Example 02</h3>
<p>On the other hand, calibration methods <span class="citation" data-cites="Tarazona2021">(<a href="#ref-Tarazona2021" role="doc-biblioref">Tarazona et al. 2021</a>)</span> such as Leave One Out Cross-Validation (LOOCV), Cross-Validation (CV) and Monte Carlos Cross-Validation (MCCV) are embedded in this python package. In this second example, MCCV will be used in order to calibrate a supervised classification with different algorithms.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 01. Endmembers</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>path_endm <span class="op">=</span> <span class="st">"\data\endmembers\endmembers.dbf"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>endm <span class="op">=</span> DBF(path_endm)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 02. Instance of calmla()</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>inst <span class="op">=</span> calmla(endmembers <span class="op">=</span> endm)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 03. Applying the splitData() method</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> inst.splitData()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Calibrating with <em>Monte Carlo Cross-Validation Calibration</em> (MCCV)</strong></p>
<p><strong>Parameters</strong>: - <code>split_data</code>: A instance obtaind with <code>splitData()</code> - <code>models</code>: Support Vector Machine (svm), Decision Tree (dt), Random Forest (rf) and Naive Bayes (nb) - <code>n_iter</code>: Number of iterations</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 04. Running MCCV</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>error_mccv <span class="op">=</span> inst.MCCV(split_data <span class="op">=</span> data, models <span class="op">=</span> (<span class="st">'svm'</span>, <span class="st">'dt'</span>, <span class="st">'rf'</span>, <span class="st">'nb'</span>), n_iter <span class="op">=</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Calibration results:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="scikit_eo_01.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Original image and Image classified in the left and right panel respectively. </figcaption><p></p>
</figure>
</div>
</section>
<section id="example-03" class="level3">
<h3 class="anchored" data-anchor-id="example-03">Example 03</h3>
<p>In this example we cover the topic of fusion of images with different observation geometry and that record information in different ranges of the electromagnetic spectrum <span class="citation" data-cites="Tarazona2021">(<a href="#ref-Tarazona2021" role="doc-biblioref">Tarazona et al. 2021</a>)</span>. The fusion of radar and optical images, although well used to improve land cover mapping, has so far not been developed tools to discuss the contributions of both images in data fusion. In <code>scikit-eo</code> we developed the function <code>fusionrs()</code> which provides us with a dictionary with the following image fusion interpretation features:</p>
<ul>
<li><em>Fused_images</em>: The fusion of both images into a 3d array</li>
<li><em>Variance</em>: The variance obtained</li>
<li><em>Proportion_of_variance</em>: The proportion of the obtained variance</li>
<li><em>Cumulative_variance</em>: The cumulative variance</li>
<li><em>Correlation</em>: Correlation of the original bands with the principal components</li>
<li><em>Contributions_in_%</em>: The contributions of each optical and radar band in the fusion</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 01 </span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>path_optical <span class="op">=</span> <span class="st">"/content/drive/MyDrive/Packages/scikit-eo_data/04_fusionrs/LC08_003069_20180906.tif"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>optical <span class="op">=</span> rasterio.<span class="bu">open</span>(path_optical)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>path_radar <span class="op">=</span> <span class="st">"/content/drive/MyDrive/Packages/scikit-eo_data/04_fusionrs/S1_2018_VV_VH.tif"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>radar <span class="op">=</span> rasterio.<span class="bu">open</span>(path_radar)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 02 Applying the fusionrs:</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>fusion <span class="op">=</span> fusionrs(optical <span class="op">=</span> optical, radar <span class="op">=</span> radar)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 03 Dictionary of results:</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>fusion.keys()</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 04 Proportion of variance:</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>prop_var <span class="op">=</span> fusion.get(<span class="st">'Proportion_of_variance'</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 05 Cumulative variance (%):</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>cum_var <span class="op">=</span> fusion.get(<span class="st">'Cumulative_variance'</span>)<span class="op">*</span><span class="dv">100</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 06 Showing the proportion of variance and cumulative:</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>x_labels <span class="op">=</span> [<span class="st">'PC</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(i<span class="op">+</span><span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(prop_var))]</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">6</span>,<span class="dv">5</span>))</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>ln1 <span class="op">=</span> axes.plot(x_labels, prop_var, marker <span class="op">=</span><span class="st">'o'</span>, markersize <span class="op">=</span> <span class="dv">6</span>,  label <span class="op">=</span> <span class="st">'Proportion of variance'</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>axes2 <span class="op">=</span> axes.twinx()</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>ln2 <span class="op">=</span> axes2.plot(x_labels, cum_var, marker <span class="op">=</span> <span class="st">'o'</span>, color <span class="op">=</span> <span class="st">'r'</span>, label <span class="op">=</span> <span class="st">"Cumulative variance"</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>ln <span class="op">=</span> ln1 <span class="op">+</span> ln2</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>labs <span class="op">=</span> [l.get_label() <span class="cf">for</span> l <span class="kw">in</span> ln]</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>axes.legend(ln, labs, loc <span class="op">=</span> <span class="st">'center right'</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>axes.set_xlabel(<span class="st">"Principal Component"</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>axes.set_ylabel(<span class="st">"Proportion of Variance"</span>)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>axes2.set_ylabel(<span class="st">"Cumulative (%)"</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>axes2.grid(<span class="va">False</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="scikit_eo_02.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Proportion of Variance and acumulative. </figcaption><p></p>
</figure>
</div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 07 Contributions of each variable in %:</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>fusion.get(<span class="st">'Contributions_in_%'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="scikit_eo_03.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Contributions. </figcaption><p></p>
</figure>
</div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 08 Preparing the image:</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>arr <span class="op">=</span> fusion.get(<span class="st">'Fused_images'</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stretch_percentiles(arr):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    p10 <span class="op">=</span> np.percentile(arr, <span class="dv">10</span>) <span class="co"># percentile10</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    p90 <span class="op">=</span> np.percentile(arr, <span class="dv">90</span>) <span class="co"># percentile90</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    clipped_arr <span class="op">=</span> np.clip(arr, p10, p90)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> (clipped_arr <span class="op">-</span> p10)<span class="op">/</span>(p90 <span class="op">-</span> p10)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> img</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>arr_fusion <span class="op">=</span> stretch_percentiles(arr)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co">## Let´s plot</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(figsize <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>axes.imshow(arr_fusion[:,:,<span class="dv">0</span>:<span class="dv">3</span>])</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>axes.set_title(<span class="st">"Fusion of optical and radar images"</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>axes.grid(<span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="scikit_eo_04.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Fusion of optical and radar images. </figcaption><p></p>
</figure>
</div>
</section>
<section id="example-04" class="level3">
<h3 class="anchored" data-anchor-id="example-04">Example 04</h3>
<p>In this final example, the assessing accuracy and area estimate will be obtained following guidance proposed by <span class="citation" data-cites="OLOFSSON201442">(<a href="#ref-OLOFSSON201442" role="doc-biblioref">Olofsson et al. 2014</a>)</span>. All that we need is both the confusion matrix and a previously obtained predicted class map.</p>
<p>Paramaters:</p>
<ul>
<li><em>matrix</em>: confusion matrix or error matrix in numpy.ndarray.</li>
<li><em>image_pred</em>: Array with 2d (rows, cols). This array should be the image classified with predicted classes.</li>
<li><em>pixel_size</em>: Pixel size of the image classified. By default is 10m of Sentinel-2. In this case is 30m (Landsat).</li>
<li><em>conf</em>: Confidence interval. By default is 95% (1.96).</li>
<li><em>nodata</em>: Nodata must be specified as 0, NaN or other any value. Keep in mind with this parameter.</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 01 load raster data</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>path_raster <span class="op">=</span> <span class="vs">r"C:\data\ml\predicted_map.tif"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> rasterio.<span class="bu">open</span>(path_optical).read(<span class="dv">1</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 02 load confusion matrix as .csv</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>path_cm <span class="op">=</span> <span class="vs">r"C:\data\ml\confusion_matrix.csv"</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> pd.read_csv(path_radar)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 03 Applying the confintervalML:</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>confintervalML(matrix <span class="op">=</span> values, image_pred <span class="op">=</span> img, pixel_size <span class="op">=</span> <span class="dv">30</span>, conf <span class="op">=</span> <span class="fl">1.96</span>, nodata <span class="op">=</span> <span class="op">-</span><span class="dv">9999</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Results:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="scikit_eo_05.png" class="img-fluid figure-img" style="width:30.0%"></p>
<p></p><figcaption class="figure-caption">Estimating area and uncertainty with 95%. </figcaption><p></p>
</figure>
</div>
<!-- #region -->
</section>
</section>
</section>
<section id="acknowledgments" class="level1">
<h1>Acknowledgments</h1>
<p>The authors would like to thank to David Montero Loaiza for the idea of the package name and Qiusheng Wu for the suggestions that helped to improve the package.</p>
</section>
<section id="references" class="level1">

<!-- #endregion -->

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Seham2022" class="csl-entry" role="doc-biblioentry">
Al-Alola, Seham S., Ibtesam I. Alkadi, Haya M. Alogayell, Soha A. Mohamed, and Ismail Y. Ismail. 2022. <span>“Air Quality Estimation Using Remote Sensing and GIS-Spatial Technologies Along Al-Shamal Train Pathway, Al-Qurayyat City in Saudi Arabia.”</span> <em>Environmental and Sustainability Indicators</em> 15 (September): 100184. <a href="https://doi.org/10.1016/J.INDIC.2022.100184">https://doi.org/10.1016/J.INDIC.2022.100184</a>.
</div>
<div id="ref-Blanc2016" class="csl-entry" role="doc-biblioentry">
Blanc, Lilian, Valery Gond, and Dinh Ho Tong Minh. 2016. <span>“Remote Sensing and Measuring Deforestation.”</span> <em>Land Surface Remote Sensing: Environment and Risks</em>, January, 27–53. <a href="https://doi.org/10.1016/B978-1-78548-105-5.50002-5">https://doi.org/10.1016/B978-1-78548-105-5.50002-5</a>.
</div>
<div id="ref-Jeannine2022" class="csl-entry" role="doc-biblioentry">
Cavender-Bares, Jeannine, Fabian D. Schneider, Maria João Santos, Amanda Armstrong, Ana Carnaval, Kyla M. Dahlin, Lola Fatoyinbo, et al. 2022. <span>“Integrating Remote Sensing with Ecology and Evolution to Advance Biodiversity Conservation.”</span> <em>Nature Ecology &amp; Evolution 2022 6:5</em> 6 (March): 506–19. <a href="https://doi.org/10.1038/s41559-022-01702-5">https://doi.org/10.1038/s41559-022-01702-5</a>.
</div>
<div id="ref-Jensen2000" class="csl-entry" role="doc-biblioentry">
Jensen, J. R. 2000. <span>“Remote Sensing of the Environment: An Earth Resource Perspective.”</span>
</div>
<div id="ref-Kucharczyk2021" class="csl-entry" role="doc-biblioentry">
Kucharczyk, Maja, and Chris H. Hugenholtz. 2021. <span>“Remote Sensing of Natural Hazard-Related Disasters with Small Drones: Global Trends, Biases, and Research Opportunities.”</span> <em>Remote Sensing of Environment</em> 264 (October): 112577. <a href="https://doi.org/10.1016/J.RSE.2021.112577">https://doi.org/10.1016/J.RSE.2021.112577</a>.
</div>
<div id="ref-Liping2018" class="csl-entry" role="doc-biblioentry">
Liping, Chen, Sun Yujun, and Sajjad Saeed. 2018. <span>“Monitoring and Predicting Land Use and Land Cover Changes Using Remote Sensing and GIS Techniques—a Case Study of a Hilly Area, Jiangle, China.”</span> <em>PLOS ONE</em> 13 (July): e0200493. <a href="https://doi.org/10.1371/JOURNAL.PONE.0200493">https://doi.org/10.1371/JOURNAL.PONE.0200493</a>.
</div>
<div id="ref-OLOFSSON201442" class="csl-entry" role="doc-biblioentry">
Olofsson, Pontus, Giles M. Foody, Martin Herold, Stephen V. Stehman, Curtis E. Woodcock, and Michael A. Wulder. 2014. <span>“Good Practices for Estimating Area and Assessing Accuracy of Land Change.”</span> <em>Remote Sensing of Environment</em> 148: 42–57. https://doi.org/<a href="https://doi.org/10.1016/j.rse.2014.02.015">https://doi.org/10.1016/j.rse.2014.02.015</a>.
</div>
<div id="ref-Tarazona2022" class="csl-entry" role="doc-biblioentry">
Tarazona, Yonatan. 2022. <span>“Mapping Deforestation Using Fractions Indices and the Non-Seasonal PVts-β Detection Approach.”</span> <em>IEEE Geoscience and Remote Sensing Letters</em> 19: 1–5. <a href="https://doi.org/10.1109/LGRS.2021.3137277">https://doi.org/10.1109/LGRS.2021.3137277</a>.
</div>
<div id="ref-Tarazona2021" class="csl-entry" role="doc-biblioentry">
Tarazona, Yonatan, Zabala Alaitz, Pons Xavier, Broquetas Antoni, Nowosad Jakub, and Zurqani Hamdi A. 2021. <span>“Fusing Landsat and SAR Data for Mapping Tropical Deforestation Through Machine Learning Classification and the PVts-β Non-Seasonal Detection Approach.”</span> <em>Canadian Journal of Remote Sensing</em> 47: 677–96. <a href="https://doi.org/10.1080/07038992.2021.1941823">https://doi.org/10.1080/07038992.2021.1941823</a>.
</div>
<div id="ref-TARAZONA2018367" class="csl-entry" role="doc-biblioentry">
Tarazona, Yonatan, Vasco M. Mantas, and A. J. S. C. Pereira. 2018. <span>“Improving Tropical Deforestation Detection Through Using Photosynthetic Vegetation Time Series – (PVts-β).”</span> <em>Ecological Indicators</em> 94: 367–79. https://doi.org/<a href="https://doi.org/10.1016/j.ecolind.2018.07.012">https://doi.org/10.1016/j.ecolind.2018.07.012</a>.
</div>
<div id="ref-Trinder2020" class="csl-entry" role="doc-biblioentry">
Trinder, John, and Qingxiang Liu. 2020. <span>“Assessing Environmental Impacts of Urban Growth Using Remote Sensing.”</span> <em>Http://Www.tandfonline.com/Action/journalInformation?show=aimsScope&amp;journalCode=tgsi20#.VsXpLiCLRhE</em> 23 (January): 20–39. <a href="https://doi.org/10.1080/10095020.2019.1710438">https://doi.org/10.1080/10095020.2019.1710438</a>.
</div>
<div id="ref-Wang2019" class="csl-entry" role="doc-biblioentry">
Wang, Xiaoyan, and Wu Yang. 2019. <span>“Water Quality Monitoring and Evaluation Using Remote-Sensing Techniques in China: A Systematic Review.”</span> <em>Ecosystem Health and Sustainability</em> 5 (January): 47–56. <a href="https://doi.org/10.1080/20964129.2019.1571443/SUPPL_FILE/TEHS_A_1571443_SM1265.DOCX">https://doi.org/10.1080/20964129.2019.1571443/SUPPL_FILE/TEHS_A_1571443_SM1265.DOCX</a>.
</div>
<div id="ref-Yang2013" class="csl-entry" role="doc-biblioentry">
Yang, Jun, Peng Gong, Rong Fu, Minghua Zhang, Jingming Chen, Shunlin Liang, Bing Xu, Jiancheng Shi, and Robert Dickinson. 2013. <span>“The Role of Satellite Remote Sensing in Climate Change Studies.”</span> <em>Nature Climate Change 2013 3:10</em> 3 (September): 875–83. <a href="https://doi.org/10.1038/nclimate1908">https://doi.org/10.1038/nclimate1908</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>